base feature:
	learning rate: 0.0001
	reward_distance: 0.8
	buffer size: 1000000


def main():
    config = {
        'num_steps': 2000,
        'robot_base': 'xmls/car.xml',
        'task': 'goal',
        'placements_extents': [-4, -4, 4, 4],
        'observation_flatten': False,
        'sensors_obs': [],

        'observe_pillars': True,
        'observe_vision': True,
        'vision_size': (84, 84),

        'constrain_pillars': True,

        'reward_distance': 0.8,  # if reward_distance is 0, then the reward function is sparse

        'lidar_max_dist': 8,
        'lidar_num_bins': 120,

        'pillars_num': 10,
        'pillars_locations': [[ 3.52087172, -3.29627743],
       [-3.60993032,  3.76972334],
       [ 0.40512185, -1.11828606],
       [-2.41838798,  0.82712593],
       [-3.68081797, -2.80101914],
       [ 1.32806649, -1.6491154 ],
       [-1.33992617,  2.2909115 ],
       [ 1.05154836,  2.05283816],
       [-1.4235473 , -1.75820755],
       [-2.50183935,  3.20091906],
       ]
    }

    env = Engine(config)
    seed = 25437624
    env.seed(seed)
    mx.random.seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    ctx = gb.try_gpu()
    ctx = mx.cpu()
    max_episodes = 1000
    max_episode_steps = 1000
    _time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))
    agent = TD3(action_dim=int(env.action_space.shape[0]),
                actor_learning_rate=0.0001,
                critic_learning_rate=0.0001,
                batch_size=64,
                memory_size=1000000,
                gamma=0.99,
                tau=0.005,
                explore_steps=10000,
                policy_update=2,
                policy_noise=0.2,
                explore_noise=0.1,
                noise_clip=0.5,
                ctx=ctx)
    mode = 'train'
    if mode == 'train':
        os.mkdir('%s' % _time)
        render = False
        episode = 0
        while agent.total_steps < 300002:
            episode += 1
            state = env.reset()
            vision = state['vision'].transpose((2, 0, 1))
            lidar = state['pillars_lidar']
            state = [vision, lidar]
            flag = False
            for step in range(max_episode_steps):
                if render:
                    env.render()
                if agent.total_steps < agent.explore_steps:
                    action = env.action_space.sample()
                    agent.total_steps += 1
                else:
                    action = agent.choose_action_train(state)
                    action = action.asnumpy()
                    agent.total_steps += 1
                next_state, reward, done, info = env.step(action)
                next_vision = next_state['vision'].transpose((2, 0, 1))
                next_lidar = next_state['pillars_lidar']
                next_state = [next_vision, next_lidar]
                if agent.total_steps % 20000 == 0:
                    agent.save(_time)
                if info['cost_pillars'] != 0:
                    reward -= 1
                    done = True
                if 'goal_met' in info.keys():
                    reward += 1
                    done = True
                    flag = True
                agent.memory_buffer.store_transition(state, action, reward, next_state, done)
                state = next_state
                if agent.total_steps > agent.explore_steps:
                    agent.update()
                if done:
                    break
            if not flag:
                print('episode %d fails at steps %d' % (episode, agent.total_steps))
            if flag:
                print('episode %d succeeds at steps %d' % (episode, agent.total_steps))

